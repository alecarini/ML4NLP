{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-JAEgM09xrRe"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "#functions to open files, merging them, splitting them into lines and then into a dataframe\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r') as file:\n",
        "        return file.read()\n",
        "\n",
        "# Function to combine contents of the two files, creating a new one\n",
        "def combine_files(file1, file2, output_file):\n",
        "    text1 = read_file(file1)\n",
        "    text2 = read_file(file2)\n",
        "    combined = text1 + \"\\n\" + text2\n",
        "\n",
        "    with open(output_file, 'w') as file:\n",
        "        file.write(combined)\n",
        "    return combined\n",
        "\n",
        "file1_path = '/content/TrainingDataNegative.txt'\n",
        "file2_path = '/content/TrainingDataPositive.txt'\n",
        "output_file_path = 'reviews.txt'\n",
        "\n",
        "combined = combine_files(file1_path, file2_path, output_file_path)\n",
        "lines = combined.split('\\n')\n",
        "df = pd.DataFrame(lines, columns=['review'])\n",
        "\n",
        "# Preprocessing functions and application\n",
        "def clean_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    text = ' '.join(word for word in text.split() if word not in stopwords.words('english'))\n",
        "    return text\n",
        "\n",
        "df['cleaned_review'] = df['review'].apply(clean_text)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(df['cleaned_review'])\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned_review'])\n",
        "max_length = 100\n",
        "X = pad_sequences(sequences, maxlen=max_length)\n",
        "y = to_categorical(df['sentiment'])\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model creation\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=10000, output_dim=64, input_length=max_length),\n",
        "    Bidirectional(LSTM(64, return_sequences=True)),\n",
        "    Dropout(0.5),\n",
        "    LSTM(32),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "model_checkpoint = ModelCheckpoint('best_model.h5', save_best_only=True, monitor='val_accuracy', mode='max')\n",
        "\n",
        "# Train model\n",
        "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_val, y_val), callbacks=[early_stopping, model_checkpoint])\n",
        "\n",
        "# Evaluate model on test data\n",
        "test_reviews = [\"This is a fantastic product!\", \"I did not like the service.\"]\n",
        "test_sequences = tokenizer.texts_to_sequences(test_reviews)\n",
        "X_test = pad_sequences(test_sequences, maxlen=max_length)\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00cd90b5-9b09-4db0-ca1b-539e6f9ab781",
        "collapsed": true,
        "id": "thWbpYLR_5is"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "282/282 [==============================] - 92s 309ms/step - loss: 0.6936 - accuracy: 0.4992 - val_loss: 0.6933 - val_accuracy: 0.4991\n",
            "Epoch 2/20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "282/282 [==============================] - 83s 294ms/step - loss: 0.6936 - accuracy: 0.5210 - val_loss: 0.6944 - val_accuracy: 0.4966\n",
            "Epoch 3/20\n",
            "282/282 [==============================] - 84s 300ms/step - loss: 0.6768 - accuracy: 0.5742 - val_loss: 0.7117 - val_accuracy: 0.5113\n",
            "Epoch 4/20\n",
            "282/282 [==============================] - 85s 301ms/step - loss: 0.6478 - accuracy: 0.6101 - val_loss: 0.7282 - val_accuracy: 0.5114\n",
            "1/1 [==============================] - 1s 855ms/step\n",
            "[[[0.5010172 ]\n",
            "  [0.5012164 ]\n",
            "  [0.5012599 ]\n",
            "  [0.50123596]\n",
            "  [0.50118697]\n",
            "  [0.5011326 ]\n",
            "  [0.5010816 ]\n",
            "  [0.50103706]\n",
            "  [0.50099975]\n",
            "  [0.50096923]\n",
            "  [0.50094485]\n",
            "  [0.5009255 ]\n",
            "  [0.50091034]\n",
            "  [0.5008986 ]\n",
            "  [0.50088954]\n",
            "  [0.5008828 ]\n",
            "  [0.5008776 ]\n",
            "  [0.50087374]\n",
            "  [0.500871  ]\n",
            "  [0.5008689 ]\n",
            "  [0.5008675 ]\n",
            "  [0.5008664 ]\n",
            "  [0.50086576]\n",
            "  [0.5008653 ]\n",
            "  [0.500865  ]\n",
            "  [0.50086486]\n",
            "  [0.50086474]\n",
            "  [0.50086474]\n",
            "  [0.50086474]\n",
            "  [0.50086474]\n",
            "  [0.5008648 ]\n",
            "  [0.50086486]\n",
            "  [0.50086486]\n",
            "  [0.5008649 ]\n",
            "  [0.500865  ]\n",
            "  [0.500865  ]\n",
            "  [0.50086504]\n",
            "  [0.5008651 ]\n",
            "  [0.5008651 ]\n",
            "  [0.5008651 ]\n",
            "  [0.50086516]\n",
            "  [0.50086516]\n",
            "  [0.50086516]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.50086516]\n",
            "  [0.50086516]\n",
            "  [0.5008651 ]\n",
            "  [0.5008651 ]\n",
            "  [0.5008651 ]\n",
            "  [0.5008651 ]\n",
            "  [0.50086504]\n",
            "  [0.500865  ]\n",
            "  [0.500865  ]\n",
            "  [0.5008649 ]\n",
            "  [0.50086486]\n",
            "  [0.50086486]\n",
            "  [0.5008648 ]\n",
            "  [0.5008648 ]\n",
            "  [0.5008648 ]\n",
            "  [0.50086486]\n",
            "  [0.50086486]\n",
            "  [0.50086504]\n",
            "  [0.5008652 ]\n",
            "  [0.5008656 ]\n",
            "  [0.50086606]\n",
            "  [0.5008667 ]\n",
            "  [0.50086766]\n",
            "  [0.5008689 ]\n",
            "  [0.5008706 ]\n",
            "  [0.50087273]\n",
            "  [0.50087565]\n",
            "  [0.50087935]\n",
            "  [0.5008841 ]\n",
            "  [0.50089014]\n",
            "  [0.50089777]\n",
            "  [0.50090736]\n",
            "  [0.5009194 ]\n",
            "  [0.5009344 ]\n",
            "  [0.5009531 ]\n",
            "  [0.5009764 ]\n",
            "  [0.5010055 ]\n",
            "  [0.5010418 ]\n",
            "  [0.5010875 ]\n",
            "  [0.5011454 ]\n",
            "  [0.5012191 ]\n",
            "  [0.5013136 ]\n",
            "  [0.50143594]\n",
            "  [0.50159484]\n",
            "  [0.5018022 ]\n",
            "  [0.50207263]\n",
            "  [0.50242406]\n",
            "  [0.5028762 ]\n",
            "  [0.5033011 ]\n",
            "  [0.5039695 ]]\n",
            "\n",
            " [[0.5010172 ]\n",
            "  [0.5012164 ]\n",
            "  [0.5012599 ]\n",
            "  [0.50123596]\n",
            "  [0.50118697]\n",
            "  [0.5011326 ]\n",
            "  [0.5010816 ]\n",
            "  [0.50103706]\n",
            "  [0.50099975]\n",
            "  [0.50096923]\n",
            "  [0.50094485]\n",
            "  [0.5009255 ]\n",
            "  [0.50091034]\n",
            "  [0.5008986 ]\n",
            "  [0.50088954]\n",
            "  [0.5008828 ]\n",
            "  [0.5008776 ]\n",
            "  [0.50087374]\n",
            "  [0.500871  ]\n",
            "  [0.5008689 ]\n",
            "  [0.5008675 ]\n",
            "  [0.5008664 ]\n",
            "  [0.50086576]\n",
            "  [0.5008653 ]\n",
            "  [0.500865  ]\n",
            "  [0.50086486]\n",
            "  [0.50086474]\n",
            "  [0.50086474]\n",
            "  [0.50086474]\n",
            "  [0.50086474]\n",
            "  [0.5008648 ]\n",
            "  [0.50086486]\n",
            "  [0.50086486]\n",
            "  [0.5008649 ]\n",
            "  [0.500865  ]\n",
            "  [0.500865  ]\n",
            "  [0.5008651 ]\n",
            "  [0.5008651 ]\n",
            "  [0.5008651 ]\n",
            "  [0.5008651 ]\n",
            "  [0.50086516]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008652 ]\n",
            "  [0.5008653 ]\n",
            "  [0.5008653 ]\n",
            "  [0.5008653 ]\n",
            "  [0.50086534]\n",
            "  [0.50086534]\n",
            "  [0.50086534]\n",
            "  [0.50086534]\n",
            "  [0.5008654 ]\n",
            "  [0.50086546]\n",
            "  [0.50086546]\n",
            "  [0.5008656 ]\n",
            "  [0.50086564]\n",
            "  [0.5008657 ]\n",
            "  [0.5008659 ]\n",
            "  [0.50086606]\n",
            "  [0.5008663 ]\n",
            "  [0.50086653]\n",
            "  [0.5008669 ]\n",
            "  [0.50086737]\n",
            "  [0.50086784]\n",
            "  [0.50086856]\n",
            "  [0.50086933]\n",
            "  [0.50087035]\n",
            "  [0.50087154]\n",
            "  [0.5008731 ]\n",
            "  [0.500875  ]\n",
            "  [0.5008774 ]\n",
            "  [0.5008803 ]\n",
            "  [0.50088394]\n",
            "  [0.50088847]\n",
            "  [0.5008943 ]\n",
            "  [0.5009018 ]\n",
            "  [0.5009115 ]\n",
            "  [0.5009242 ]\n",
            "  [0.5009412 ]\n",
            "  [0.5009643 ]\n",
            "  [0.50099564]\n",
            "  [0.50103885]\n",
            "  [0.50109863]\n",
            "  [0.5011816 ]\n",
            "  [0.5012963 ]\n",
            "  [0.5014545 ]\n",
            "  [0.50167006]\n",
            "  [0.5019599 ]\n",
            "  [0.50234115]\n",
            "  [0.502827  ]\n",
            "  [0.5034171 ]\n",
            "  [0.50440484]\n",
            "  [0.505305  ]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cTvW1yXSA9fs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assign binary labels for sentiment (0 for negative, 1 for positive)\n",
        "df['sentiment'] = np.random.randint(2, size=len(df))\n",
        "\n",
        "# Tokenize and pad sequences\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(df['cleaned_review'])\n",
        "sequences = tokenizer.texts_to_sequences(df['cleaned_review'])\n",
        "max_length = 100\n",
        "X = pad_sequences(sequences, maxlen=max_length)\n",
        "y = to_categorical(df['sentiment'])\n",
        "\n",
        "# Split data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "aUtK4MamscZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "val_predictions = model.predict(X_val)\n",
        "val_pred_labels = np.argmax(val_predictions, axis=1)\n",
        "val_true_labels = np.argmax(y_val, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(val_true_labels, val_pred_labels)\n",
        "precision = precision_score(val_true_labels, val_pred_labels)\n",
        "\n",
        "print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Validation Precision: {precision:.4f}\")\n",
        "\n",
        "# Testing the model on a couple of sample sentences\n",
        "sample_sentences = [\"I love this product!\", \"This is the worst service I have ever received.\"]\n",
        "cleaned_samples = [clean_text(sentence) for sentence in sample_sentences]\n",
        "sample_sequences = tokenizer.texts_to_sequences(cleaned_samples)\n",
        "sample_padded = pad_sequences(sample_sequences, maxlen=max_length)\n",
        "\n",
        "sample_predictions = model.predict(sample_padded)\n",
        "sample_pred_labels = np.argmax(sample_predictions, axis=1)\n",
        "\n",
        "for sentence, label in zip(sample_sentences, sample_pred_labels):\n",
        "    sentiment = \"Positive\" if label == 1 else \"Negative\"\n",
        "    print(f\"Sentence: '{sentence}' - Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "id": "xCH1FCSoBUOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score\n",
        "\n",
        "num_negative_reviews = sum(1 for line in open(file1_path))\n",
        "num_positive_reviews = sum(1 for line in open(file2_path))\n",
        "labels = [0] * num_negative_reviews + [1] * num_positive_reviews\n",
        "df['label'] = labels\n",
        "\n",
        "#split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(df['cleaned_review'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = CountVectorizer()\n",
        "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
        "X_test_vectorized = vectorizer.transform(X_test)\n",
        "\n",
        "# Train and evaluate model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_vectorized, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test_vectorized)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print(f'Precision: {precision}')\n",
        "\n",
        "# Example review to test\n",
        "example_review = \"I love this product, i've been wasting my money on other stuff for so long.\"\n",
        "cleaned_example_review = clean_text(example_review)\n",
        "vectorized_example_review = vectorizer.transform([cleaned_example_review])\n",
        "predicted_score = model.predict(vectorized_example_review)\n",
        "\n",
        "print(f\"Original Review: {example_review}\")\n",
        "print(f\"Cleaned Review: {cleaned_example_review}\")\n",
        "print(f\"Predicted Sentiment Score: {predicted_score[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "3i0OQrEXEtb6",
        "outputId": "e7b98179-ea7b-4807-eb55-ae6fbf377164"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/TrainingDataNegative.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-44062e7adc24>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnum_negative_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile1_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mnum_positive_reviews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile2_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_negative_reviews\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_positive_reviews\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/TrainingDataNegative.txt'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KNsftvnNKOLf"
      },
      "execution_count": 4,
      "outputs": []
    }
  ]
}